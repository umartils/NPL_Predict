# -*- coding: utf-8 -*-
"""NPL_Predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yyepnXJX74nvNUADMZ8igHu9DMCw4Sac
"""

!pip install kagglehub

"""# Import Library"""

import kagglehub
import os
import shutil

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import math
import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix, classification_report

path = kagglehub.dataset_download("architsharma01/loan-approval-prediction-dataset")

print("Konten direktori utama:")
print(os.listdir(path))
for root, dirs, _ in os.walk(path):
    print(root)

df = pd.read_csv(f"{path}/loan_approval_dataset.csv")
df.columns = df.columns.str.strip()
df.head()

"""# EDA

## Data Understanding
"""

df.info()

df.describe().T

null_val = df.isna().sum()
duplicated_data = df.duplicated().sum()
print("===============================")
print(f"Jumlah Missing Values: \n{null_val}")
print(f"Jumlah Data Duplikat: {duplicated_data}")
print("===============================")

"""## Univariate Analysis"""

data = df.copy()
data = data.drop(columns='loan_id')
num_col = data.select_dtypes(include=['int64']).columns
cat_col = data.select_dtypes(include='object').columns

# Pilih palet warna
palette = sns.color_palette("Set2")

# Tentukan jumlah subplot berdasarkan jumlah kolom kategorikal
n_col = len(cat_col)
n_rows = (n_col + 2) // 2  # 2 kolom per baris
fig, axs = plt.subplots(1, 3, figsize=(15, 5))
axs = axs.flatten()

# Loop untuk membuat countplot
for i, kolom in enumerate(cat_col):
    ax = axs[i]
    sns.countplot(data=df, x=kolom, palette=palette, ax=ax)
    ax.set_title(f"Distribusi {kolom}", fontsize=14, fontweight='normal')
    ax.set_xlabel(kolom, fontsize=12)
    ax.set_ylabel("Jumlah", fontsize=12)
    ax.tick_params(axis='x', rotation=30)

    # Tambahkan label di atas setiap bar
    for p in ax.patches:
        height = p.get_height()
        ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., height),
                    ha='center', va='bottom', fontsize=10, color='black')

# Sembunyikan subplot yang tidak terpakai
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.suptitle("Distribusi Kolom Kategorikal", fontsize=16, fontweight='normal')
plt.subplots_adjust(wspace=0.4, top=0.8)  # atur jarak horizontal dan ruang atas
plt.show()

# Palet warna (tidak begitu berpengaruh untuk histogram, tapi tetap bisa digunakan)
palette = sns.color_palette("Set2")

# Ganti cat_col dengan num_col
n_col = len(num_col)
n_cols = 3  # jumlah kolom subplot per baris
n_rows = (n_col + n_cols - 1) // n_cols

# Buat subplots
fig, axs = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))
axs = axs.flatten()

# Loop untuk membuat histogram
for i, kolom in enumerate(num_col):
    ax = axs[i]
    sns.histplot(data=df, x=kolom, bins=20, kde=True, color=palette[i % len(palette)], ax=ax)
    ax.set_title(f"Distribusi {kolom}", fontsize=14, fontweight='normal')
    ax.set_xlabel(kolom, fontsize=12)
    ax.set_ylabel("Frekuensi", fontsize=12)
    ax.grid(True, linestyle='--', alpha=0.5)

# Sembunyikan subplot kosong (jika jumlah kolom numerik tidak habis dibagi n_cols)
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.suptitle("Distribusi Kolom Numerik", fontsize=20, fontweight='normal')
plt.subplots_adjust(wspace=0.4, hspace=0.4, top=0.9)
plt.show()

# Hitung jumlah subplot
n_cols = 3
n_rows = math.ceil(len(num_col) / n_cols)
df_long = df[num_col].melt(var_name='Feature', value_name='Value')
# Buat figure dan axes
fig, axs = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))
axs = axs.flatten()

# Buat boxplot untuk setiap fitur numerik
for i, col in enumerate(num_col):
    sns.boxplot(data=df_long[df_long['Feature'] == col], x='Feature', y='Value', ax=axs[i])
    axs[i].set_title(f'Boxplot of {col}')
    # axs[i].tick_params(axis='x', rotation=45)

# Hapus axis kosong jika ada
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.suptitle("Boxplot Kolom Numerik", fontsize=20, fontweight='normal')
plt.subplots_adjust(wspace=0.4, hspace=0.4)
plt.show()

"""## Multivariate Analysis"""

sns.pairplot(data, hue='loan_status')
plt.suptitle("Pairplot Kolom Numerik", fontsize=16, y=1.02)
plt.show()

"""## Analisis Korelasi"""

data_corr = data.copy()
data_corr['loan_status'] = data_corr['loan_status'].map({' Approved':1, ' Rejected':0})

plt.figure(figsize=(10, 8))
correlation_matrix = data_corr.corr(numeric_only=True)
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Korelasi Setiap Kolom Numerik")
plt.show()

# Salin dataframe agar tidak mengubah aslinya
df_corr = data.copy()
df_corr['loan_status'] = df_corr['loan_status'].map({' Approved': 1, ' Rejected': 0})

# Ambil hanya kolom numerik + target
corr = df_corr.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(corr[['loan_status']].sort_values(by='loan_status', ascending=False),
            annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Korelasi Fitur Numerik dengan Loan Status", fontsize=16, fontweight='normal')
plt.show()

"""# Data Pre Processing (Data Preparation)

## Data Cleaning
"""

df.describe().T

(data['residential_assets_value'] < 0).sum()

# data = df.copy()
data = data[data['residential_assets_value'] >= 0]
data.info()

"""## Mengatasi Pencilan (Outlier)"""

# data = df.drop(columns='loan_id')
numeric_columns = data.select_dtypes(include='int64').columns

for kolom in numeric_columns:
  q1 = data[kolom].quantile(0.25)
  q3 = data[kolom].quantile(0.75)
  iqr = q3-q1

  low_bound = q1 - 1.5 * iqr
  up_bound = q3 + 1.5 * iqr
  data = data[(data[kolom] > low_bound) & (data[kolom] < up_bound)]

data.info()

# Hitung jumlah subplot
n_cols = 3
n_rows = math.ceil(len(numeric_columns) / n_cols)
data_long = data[numeric_columns].melt(var_name='Feature', value_name='Value')

# Buat figure dan axes
fig, axs = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))
axs = axs.flatten()

# Buat boxplot untuk setiap fitur numerik
for i, col in enumerate(numeric_columns):
    sns.boxplot(data=data_long[data_long['Feature'] == col], x='Feature', y='Value', ax=axs[i])
    axs[i].set_title(f'Boxplot of {col}')
    # axs[i].tick_params(axis='x', rotation=45)

# Hapus axis kosong jika ada
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.suptitle("Boxplot Kolom Numerik", fontsize=20, fontweight='normal')
plt.subplots_adjust(wspace=0.4, hspace=0.4)
plt.show()

"""## Encoding Categorical Columns"""

for kolom in cat_col:
  print(f"{kolom}: {data[kolom].unique()}")

data[cat_col].head()

# data = df.drop(columns='loan_id')
data['education'] = data['education'].map({' Graduate': 1, ' Not Graduate': 0})
data['self_employed'] = data['self_employed'].map({' Yes': 1, ' No': 0})
data['loan_status'] = data['loan_status'].map({' Approved': 1, ' Rejected': 0})
data[cat_col].head()

"""## Data Standardization"""

data.describe().T

scaler = MinMaxScaler()
scaled_numeric = pd.DataFrame(
    scaler.fit_transform(data[num_col]),
    columns=num_col,
    index=data.index
)

# Gabungkan kembali dengan kolom kategorikal
data_scaled = pd.concat([scaled_numeric, data[cat_col]], axis=1)

# Tampilkan hasil
scaled_numeric.describe().T

"""## Handling Imbalanced Data (Oversampling)"""

# data = df.drop(columns='loan_id')
X = data_scaled.drop(columns='loan_status')
y = data_scaled['loan_status']

# Melihat distribusi kelas awal
count_0 = np.sum(y == 0)
count_1 = np.sum(y == 1)
print("Sebelum oversampling:")
print(f"Jumlah baris data yang bernilai '0' ada sebanyak: {count_0}")
print(f"Jumlah baris data yang bernilai '1' ada sebanyak: {count_1}")
print(f"Persentase kelas 0: {count_0/(count_0+count_1)*100:.2f}%")
print(f"Persentase kelas 1: {count_1/(count_0+count_1)*100:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns

# Salin data dan ubah label
df_imbalanced = data_scaled.copy()
df_imbalanced['loan_status'] = df_imbalanced['loan_status'].map({1: 'Approved', 0: 'Rejected'})

# Tentukan warna berdasarkan label
palette = sns.color_palette("Set2")
custom_palette = {'Approved': palette[0], 'Rejected': palette[1]}


# Plot
fig, axs = plt.subplots(figsize=(6, 5))
sns.countplot(data=df_imbalanced, x='loan_status', palette=custom_palette, ax=axs)

# Tambahkan label jumlah dan persentase
total = len(df_imbalanced)
for p in axs.patches:
    count = p.get_height()
    percentage = 100 * count / total
    axs.annotate(f'{count} ({percentage:.1f}%)',
                 (p.get_x() + p.get_width() / 2., count),
                 ha='center', va='bottom', fontsize=10, color='black')

plt.tight_layout()
plt.show()

# Over-sampling dengan keseimbangan penuh
over_sampler = RandomOverSampler(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = over_sampler.fit_resample(X, y)

# Melihat distribusi kelas setelah oversampling
count_0 = np.sum(y_resampled == 0)
count_1 = np.sum(y_resampled == 1)
print("\nSetelah oversampling:")
print(f"Jumlah baris data yang bernilai '0' ada sebanyak: {count_0}")
print(f"Jumlah baris data yang bernilai '1' ada sebanyak: {count_1}")
print(f"Persentase kelas 0: {count_0/(count_0+count_1)*100:.2f}%")
print(f"Persentase kelas 1: {count_1/(count_0+count_1)*100:.2f}%")

df_resampled = pd.DataFrame(X_resampled, columns=X.columns)

# Menambahkan kolom target dari y_resampled
df_resampled['loan_status'] = y_resampled

df_resampled.to_csv('npl_data.csv', index=False)

df_y = pd.DataFrame({'loan_status': y_resampled})
df_y['loan_status'] = df_y['loan_status'].map({1:'Approved', 0:"Rejected"})

# Plot
palette = sns.color_palette("Set2")
custom_palette = {'Approved': palette[0], 'Rejected': palette[1]}
fig, axs = plt.subplots(figsize=(6, 5))
# Pilih palet warna
sns.countplot(data=df_y, x='loan_status', palette=custom_palette)
total = len(df_y)
for p in axs.patches:
  count = p.get_height()
  percentage = 100 * count / total
  axs.annotate(f'{count} ({percentage:.1f}%)',
              (p.get_x() + p.get_width() / 2., count),
              ha='center', va='bottom', fontsize=10, color='black')

"""## Data Splitting"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)
print(X_train.shape)
print(y_train.shape)

"""# Modeling"""

# Daftar model yang digunakan
models = {
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
}

# Dictionary untuk menyimpan hasil evaluasi
results = []

# Evaluasi setiap model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    if hasattr(model, "predict_proba"):
        y_scores = model.predict_proba(X_test)[:, 1]  # Probabilitas kelas 1
    else:
        y_scores = model.decision_function(X_test)   # Untuk model SVM (jika tidak pakai probability=True)

    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC-AUC": roc_auc_score(y_test, y_scores)
    })

# Ubah hasil ke dalam bentuk DataFrame
results_df = pd.DataFrame(results)

# Tampilkan hasil
results_df = results_df.sort_values(by='Accuracy', ascending=False)
results_df = results_df.reset_index(drop=True)
results_df

# Ubah dataframe ke format long agar bisa digunakan di seaborn barplot
results_long = results_df.melt(id_vars='Model', value_vars=['Accuracy', 'F1 Score', 'ROC-AUC'],
                               var_name='Metric', value_name='Score')

# Buat bar plot
plt.figure(figsize=(10, 6))
ax = sns.barplot(data=results_long, x='Model', y='Score', hue='Metric')
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2,
            height + 0.01,
            f'{height:.3f}',
            ha='center', va='bottom', fontsize=9)
# Tambahkan judul dan label
plt.title('Accuracy and F1 Score for Setiap Model')
plt.ylabel('Score')
plt.ylim(0, 1.1)  # Tambah batas atas agar label tidak terpotong
plt.legend(title='Metric')
plt.tight_layout()
plt.show()

"""#  Hyperparameter Tuning (Grid Search)"""

# Parameter grid untuk setiap model
param_grid = {
    "SVM": {
        'C': [0.1, 1, 10],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto']
    },
    "Random Forest": {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 10],
        'min_samples_leaf': [1, 5]
    },
    "XGBoost": {
        'n_estimators': [100, 200],
        'max_depth': [3, 6, 10],
        'learning_rate': [0.01, 0.1, 0.2],
        'subsample': [0.8, 1]
    }
}

# Model dictionary
models_grid = {
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(random_state=32),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=32),
}

# Tempat menyimpan model terbaik
best_models_grid = {}

# Melakukan grid search untuk setiap model
for name in models_grid:
    print(f"Tuning model: {name}")
    grid = GridSearchCV(models_grid[name], param_grid[name], cv=5, n_jobs=-1, scoring='accuracy')

    # Gunakan data yang sudah discale untuk SVM, data asli untuk RF dan XGBoost
    grid.fit(X_train, y_train)

    print(f"Best parameters for {name}: {grid.best_params_}")
    print(f"Best score for {name}: {grid.best_score_:.4f}\n")
    best_models_grid[name] = grid.best_estimator_

for name, model in best_models_grid.items():
  y_pred = best_models_grid[name].predict(X_test)
  print(f"Test accuracy for {name}: {accuracy_score(y_test, y_pred):.4f}")
  print(f"Parameter dari model {name}:")
  print(model.get_params())
  print("-" * 50)

grid_results = []

# Evaluasi setiap model
for name, model in best_models_grid.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    grid_results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    })

grid_results_df = pd.DataFrame(grid_results)

# Tampilkan hasil
grid_results_df = grid_results_df.sort_values(by='Accuracy', ascending=False)
grid_results_df = grid_results_df.reset_index(drop=True)
grid_results_df

"""# Best Model"""

# Daftar model yang digunakan
best_models = {
    "SVM": SVC(
        C=10, kernel='rbf',
        gamma='scale',
        random_state=32),
    "Random Forest": RandomForestClassifier(random_state=32),
    "XGBoost": XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        learning_rate= 0.2,
        max_depth= 10,
        n_estimators= 100,
        subsample= 0.8,
        random_state=32),
}

# Dictionary untuk menyimpan hasil evaluasi
best_results = []

# Evaluasi setiap model
for name, model in best_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    if hasattr(model, "predict_proba"):
        y_scores = model.predict_proba(X_test)[:, 1]  # Probabilitas kelas 1
    else:
        y_scores = model.decision_function(X_test)   # Untuk model SVM (jika tidak pakai probability=True)

    fpr, tpr, _ = roc_curve(y_test, y_scores)
    auc = roc_auc_score(y_test, y_scores)
    best_results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "ROC-AUC": auc
    })
    print(f"Test accuracy for {name}: {accuracy_score(y_test, y_pred):.4f}")
    print(f"Parameter dari model {name}:")
    print(model.get_params())
    print("-" * 50)

# Ubah hasil ke dalam bentuk DataFrame
best_results_df = pd.DataFrame(best_results)

# Tampilkan hasil
best_results_df = best_results_df.sort_values(by='Accuracy', ascending=False)
best_results_df = best_results_df.reset_index(drop=True)
best_results_df

plt.figure(figsize=(8, 6))
for name, model in best_models.items():
    # Training
    model.fit(X_train, y_train)

    # Prediction
    y_pred = model.predict(X_test)

    # ROC AUC
    if hasattr(model, "predict_proba"):
        y_scores = model.predict_proba(X_test)[:, 1]  # Probabilitas kelas 1
    else:
        y_scores = model.decision_function(X_test)   # Untuk model SVM (jika tidak pakai probability=True)

    fpr, tpr, _ = roc_curve(y_test, y_scores)
    auc = roc_auc_score(y_test, y_scores)

    # Plot ROC
    plt.plot(fpr, tpr, label=f"{name} (AUC = {auc:.2f})")

# Plot layout
plt.plot([0, 1], [0, 1], 'k--')  # Garis diagonal
plt.title("ROC Curve Setiap Model")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

# Ubah dataframe ke format long agar bisa digunakan di seaborn barplot
results_long = best_results_df.melt(id_vars='Model', value_vars=['Accuracy', 'F1 Score', 'ROC-AUC'],
                               var_name='Metric', value_name='Score')

# Buat bar plot
plt.figure(figsize=(9, 7))
ax = sns.barplot(data=results_long, x='Model', y='Score', hue='Metric')
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width()/2,
            height + 0.01,
            f'{height:.3f}',
            ha='center', va='bottom', fontsize=9)
# Tambahkan judul dan label
plt.title('Accuracy, F1 Score dan ROC-AUV Setiap Model')
plt.ylabel('Score')
plt.ylim(0, 1.1)  # Tambah batas atas agar label tidak terpotong
plt.legend(title='Metric')
plt.tight_layout()
plt.show()

"""# Confusion Matrix"""

def plot_conf_matrix(y_true, y_pred, model_name,):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(7, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

for name, model in best_models.items():
    print(f"Evaluating model: {name}")
    y_pred = model.predict(X_test)

    # Print classification report
    print(classification_report(y_test, y_pred))

    # Tampilkan confusion matrix
    plot_conf_matrix(y_test, y_pred, name)

# Feature Importance

feat_importance_rf = best_models['Random Forest'].feature_importances_
features = X_train.columns

# Buat dataframe untuk tampilan yang rapi
feat_importance_rf_df = pd.DataFrame({'Feature': features, 'Importance': feat_importance_rf})
feat_importance_rf_df = feat_importance_rf_df.sort_values(by='Importance', ascending=False)
feat_importance_rf_df

feat_importance_xgb = best_models['XGBoost'].get_booster().get_score(importance_type='gain')

# Ubah ke DataFrame
feat_importance_xgb_df = pd.DataFrame.from_dict(feat_importance_xgb, orient='index', columns=['Importance'])
feat_importance_xgb_df = feat_importance_xgb_df.reset_index().rename(columns={'index': 'Feature'})
feat_importance_xgb_df = feat_importance_xgb_df.sort_values(by='Importance', ascending=False)

feat_importance_xgb_df = feat_importance_xgb_df.reset_index(drop=True)

feat_importance_xgb_df

